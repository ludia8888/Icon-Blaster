"""
gRPC server for Embedding Service
"""
import os
import grpc
import asyncio
import logging
import numpy as np
from typing import List

from grpc_reflection.v1alpha import reflection
from opentelemetry.instrumentation.grpc import server_interceptor

# Import will be generated by protoc
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from proto import embedding_service_pb2 as pb2
from proto import embedding_service_pb2_grpc as pb2_grpc

from app.embeddings.service import VectorEmbeddingService
from app.embeddings.providers import EmbeddingServiceProvider

logger = logging.getLogger(__name__)


class EmbeddingServicer(pb2_grpc.EmbeddingServiceServicer):
    """gRPC service implementation for embedding operations"""
    
    def __init__(self):
        self.provider = EmbeddingServiceProvider()
        self.service = None
        
    async def initialize(self):
        """Initialize the service"""
        await self.provider.initialize()
        self.service = await self.provider.provide()
        logger.info("Embedding gRPC service initialized")
    
    async def GenerateEmbedding(self, request: pb2.EmbeddingRequest, context: grpc.aio.ServicerContext) -> pb2.EmbeddingResponse:
        """Generate embedding for a single text"""
        try:
            import time
            start_time = time.time()
            
            # Generate embedding
            embedding = await self.service.generate_embedding(
                request.text,
                metadata={
                    "trace_id": request.meta.trace_id if request.meta else None,
                    "user_id": request.meta.user_id if request.meta else None
                }
            )
            
            processing_time = (time.time() - start_time) * 1000
            
            # Convert numpy array to list
            if isinstance(embedding, np.ndarray):
                embedding = embedding.tolist()
            
            return pb2.EmbeddingResponse(
                embedding=embedding,
                dimensions=len(embedding),
                model_used=request.model_name or "sentence-transformers/all-MiniLM-L6-v2",
                processing_time_ms=processing_time
            )
        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return pb2.EmbeddingResponse()
    
    async def GenerateBatchEmbeddings(self, request: pb2.BatchEmbeddingRequest, context: grpc.aio.ServicerContext) -> pb2.BatchEmbeddingResponse:
        """Generate embeddings for multiple texts"""
        try:
            import time
            start_time = time.time()
            
            # Generate batch embeddings
            embeddings = await self.service.generate_batch_embeddings(
                request.texts,
                metadata={
                    "trace_id": request.meta.trace_id if request.meta else None,
                    "user_id": request.meta.user_id if request.meta else None
                }
            )
            
            processing_time = (time.time() - start_time) * 1000
            
            # Convert results
            results = []
            for embedding in embeddings:
                if isinstance(embedding, np.ndarray):
                    embedding = embedding.tolist()
                results.append(pb2.EmbeddingResult(
                    embedding=embedding,
                    success=True
                ))
            
            return pb2.BatchEmbeddingResponse(
                results=results,
                model_used=request.model_name or "sentence-transformers/all-MiniLM-L6-v2",
                total_processing_time_ms=processing_time
            )
        except Exception as e:
            logger.error(f"Error generating batch embeddings: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return pb2.BatchEmbeddingResponse()
    
    async def CalculateSimilarity(self, request: pb2.SimilarityRequest, context: grpc.aio.ServicerContext) -> pb2.SimilarityResponse:
        """Calculate similarity between two embeddings"""
        try:
            # Convert lists to numpy arrays
            embedding1 = np.array(request.embedding1)
            embedding2 = np.array(request.embedding2)
            
            # Calculate similarity
            similarity = await self.service.calculate_similarity(
                embedding1,
                embedding2,
                metric=request.metric or "cosine"
            )
            
            return pb2.SimilarityResponse(
                similarity=float(similarity),
                metric_used=request.metric or "cosine"
            )
        except Exception as e:
            logger.error(f"Error calculating similarity: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return pb2.SimilarityResponse()
    
    async def FindSimilar(self, request: pb2.SimilarSearchRequest, context: grpc.aio.ServicerContext) -> pb2.SimilarSearchResponse:
        """Find similar documents"""
        try:
            import time
            start_time = time.time()
            
            # Convert query embedding
            query_embedding = np.array(request.query_embedding)
            
            # Search for similar documents
            results = await self.service.find_similar(
                query_embedding,
                collection=request.collection,
                top_k=request.top_k,
                min_similarity=request.min_similarity,
                filters=dict(request.filters) if request.filters else None
            )
            
            search_time = (time.time() - start_time) * 1000
            
            # Convert results
            documents = []
            for result in results:
                documents.append(pb2.SimilarDocument(
                    id=result.get("id", ""),
                    similarity=result.get("similarity", 0.0),
                    metadata=result.get("metadata", {})
                ))
            
            return pb2.SimilarSearchResponse(
                documents=documents,
                total_found=len(documents),
                search_time_ms=search_time
            )
        except Exception as e:
            logger.error(f"Error searching similar documents: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return pb2.SimilarSearchResponse()
    
    async def StoreEmbedding(self, request: pb2.StoreEmbeddingRequest, context: grpc.aio.ServicerContext) -> pb2.StoreEmbeddingResponse:
        """Store embedding with metadata"""
        try:
            # Convert embedding
            embedding = np.array(request.embedding)
            
            # Store embedding
            success = await self.service.store_embedding(
                id=request.id,
                embedding=embedding,
                collection=request.collection,
                metadata=dict(request.metadata) if request.metadata else {}
            )
            
            return pb2.StoreEmbeddingResponse(
                success=success,
                message="Embedding stored successfully" if success else "Failed to store embedding"
            )
        except Exception as e:
            logger.error(f"Error storing embedding: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return pb2.StoreEmbeddingResponse(success=False, message=str(e))
    
    async def HealthCheck(self, request: pb2.HealthRequest, context: grpc.aio.ServicerContext) -> pb2.HealthResponse:
        """Health check"""
        try:
            stats = await self.service.get_stats()
            
            return pb2.HealthResponse(
                healthy=True,
                model_loaded="sentence-transformers/all-MiniLM-L6-v2",
                embeddings_cached=stats.get("embeddings_cached", 0),
                details={
                    "cache_size": str(stats.get("cache_size", 0)),
                    "total_processed": str(stats.get("total_processed", 0))
                }
            )
        except Exception as e:
            return pb2.HealthResponse(
                healthy=False,
                details={"error": str(e)}
            )


async def serve(port: int = 50055):
    """Start the gRPC server"""
    # Initialize servicer
    servicer = EmbeddingServicer()
    await servicer.initialize()
    
    # Create server with OpenTelemetry interceptor
    server = grpc.aio.server(
        interceptors=[server_interceptor()]
    )
    
    # Add service
    pb2_grpc.add_EmbeddingServiceServicer_to_server(servicer, server)
    
    # Enable reflection
    SERVICE_NAMES = (
        pb2.DESCRIPTOR.services_by_name['EmbeddingService'].full_name,
        reflection.SERVICE_NAME,
    )
    reflection.enable_server_reflection(SERVICE_NAMES, server)
    
    # Start server
    server.add_insecure_port(f'[::]:{port}')
    await server.start()
    
    logger.info(f"Embedding gRPC server started on port {port}")
    
    try:
        await server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("Shutting down gRPC server...")
        await server.stop(5)


if __name__ == "__main__":
    port = int(os.getenv("GRPC_PORT", "50055"))
    asyncio.run(serve(port))