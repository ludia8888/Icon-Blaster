"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
ëŒ€ëŸ‰ ì—”í‹°í‹° ê²€ì¦, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ë™ì‹œì„± í…ŒìŠ¤íŠ¸
"""
import time
import psutil
import os
import gc
import concurrent.futures
from typing import List, Dict, Tuple
import statistics
import json
from dataclasses import dataclass

from core.validation.naming_convention import (
    NamingConventionEngine, EntityType, get_naming_engine
)
from core.validation.naming_config import get_naming_config_service


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼"""
    name: str
    total_operations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    std_dev: float
    ops_per_second: float
    memory_used_mb: float
    
    def __str__(self):
        return f"""
=== {self.name} ===
Total Operations: {self.total_operations:,}
Total Time: {self.total_time:.3f}s
Average Time: {self.avg_time * 1000:.3f}ms
Min Time: {self.min_time * 1000:.3f}ms
Max Time: {self.max_time * 1000:.3f}ms
Std Dev: {self.std_dev * 1000:.3f}ms
Operations/Second: {self.ops_per_second:,.0f}
Memory Used: {self.memory_used_mb:.2f} MB
"""


class NamingBenchmark:
    """ëª…ëª… ê·œì¹™ ê²€ì¦ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.engine = get_naming_engine()
        self.process = psutil.Process(os.getpid())
    
    def generate_test_names(self, count: int) -> List[Tuple[EntityType, str]]:
        """í…ŒìŠ¤íŠ¸ìš© ì´ë¦„ ìƒì„±"""
        names = []
        
        # ë‹¤ì–‘í•œ íŒ¨í„´ì˜ ì´ë¦„ ìƒì„±
        patterns = [
            # ìœ íš¨í•œ ì´ë¦„ë“¤
            ("Product", "ProductManager", "ProductType"),
            ("User", "UserAccount", "UserProfile"),
            ("Order", "OrderItem", "OrderStatus"),
            # ë¬´íš¨í•œ ì´ë¦„ë“¤
            ("product", "_Product", "Product_Type"),
            ("123Product", "Product!", "Pro duct"),
            # ë³µì¡í•œ ì´ë¦„ë“¤
            ("HTTPServerManager", "OAuth2Provider", "XMLHttpRequest"),
            ("APIv3Client", "DB2Connection", "HTTP2ServerError"),
        ]
        
        entity_types = [
            EntityType.OBJECT_TYPE,
            EntityType.PROPERTY,
            EntityType.LINK_TYPE,
            EntityType.FUNCTION_TYPE,
        ]
        
        # ì§€ì •ëœ ìˆ˜ë§Œí¼ ì´ë¦„ ìƒì„±
        for i in range(count):
            pattern_group = patterns[i % len(patterns)]
            name = pattern_group[i % len(pattern_group)]
            entity_type = entity_types[i % len(entity_types)]
            
            # ë³€í˜• ì¶”ê°€
            if i % 5 == 0:
                name = name.lower()
            elif i % 7 == 0:
                name = name + str(i)
            elif i % 11 == 0:
                name = "_" + name
            
            names.append((entity_type, name))
        
        return names
    
    def benchmark_single_validation(self, iterations: int = 1000) -> BenchmarkResult:
        """ë‹¨ì¼ ê²€ì¦ ì„±ëŠ¥ ì¸¡ì •"""
        test_names = self.generate_test_names(iterations)
        times = []
        
        # ì›Œë°ì—…
        for _ in range(100):
            self.engine.validate(EntityType.OBJECT_TYPE, "TestObject")
        
        # ë©”ëª¨ë¦¬ ì¸¡ì • ì‹œì‘
        gc.collect()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        total_start = time.perf_counter()
        
        for entity_type, name in test_names:
            start = time.perf_counter()
            self.engine.validate(entity_type, name)
            elapsed = time.perf_counter() - start
            times.append(elapsed)
        
        total_time = time.perf_counter() - total_start
        
        # ë©”ëª¨ë¦¬ ì¸¡ì • ì¢…ë£Œ
        end_memory = self.process.memory_info().rss / 1024 / 1024
        memory_used = end_memory - start_memory
        
        return BenchmarkResult(
            name="Single Validation Benchmark",
            total_operations=iterations,
            total_time=total_time,
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            std_dev=statistics.stdev(times) if len(times) > 1 else 0,
            ops_per_second=iterations / total_time,
            memory_used_mb=memory_used
        )
    
    def benchmark_10k_entities(self) -> BenchmarkResult:
        """10,000ê°œ ì—”í‹°í‹° ê²€ì¦ ì„±ëŠ¥ ì¸¡ì •"""
        test_names = self.generate_test_names(10000)
        
        gc.collect()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        
        start_time = time.perf_counter()
        
        for entity_type, name in test_names:
            self.engine.validate(entity_type, name)
        
        total_time = time.perf_counter() - start_time
        
        end_memory = self.process.memory_info().rss / 1024 / 1024
        memory_used = end_memory - start_memory
        
        return BenchmarkResult(
            name="10K Entities Benchmark",
            total_operations=10000,
            total_time=total_time,
            avg_time=total_time / 10000,
            min_time=total_time / 10000,  # í‰ê· ê°’ ì‚¬ìš©
            max_time=total_time / 10000,
            std_dev=0,
            ops_per_second=10000 / total_time,
            memory_used_mb=memory_used
        )
    
    def benchmark_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§"""
        results = {}
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬
        gc.collect()
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        results['initial_mb'] = initial_memory
        
        # ì—”ì§„ ìƒì„± í›„
        engine = get_naming_engine()
        after_engine = self.process.memory_info().rss / 1024 / 1024
        results['after_engine_mb'] = after_engine
        results['engine_overhead_mb'] = after_engine - initial_memory
        
        # 1000ë²ˆ ê²€ì¦ í›„
        for i in range(1000):
            engine.validate(EntityType.OBJECT_TYPE, f"TestObject{i}")
        
        after_1k = self.process.memory_info().rss / 1024 / 1024
        results['after_1k_validations_mb'] = after_1k
        results['validation_overhead_mb'] = after_1k - after_engine
        
        # ìºì‹œ íš¨ê³¼ ì¸¡ì • (ê°™ì€ ì´ë¦„ ë°˜ë³µ)
        for _ in range(1000):
            engine.validate(EntityType.OBJECT_TYPE, "CachedObject")
        
        after_cache = self.process.memory_info().rss / 1024 / 1024
        results['after_cache_test_mb'] = after_cache
        results['cache_overhead_mb'] = after_cache - after_1k
        
        # GC í›„
        gc.collect()
        after_gc = self.process.memory_info().rss / 1024 / 1024
        results['after_gc_mb'] = after_gc
        results['gc_freed_mb'] = after_cache - after_gc
        
        return results
    
    def benchmark_concurrent_validation(self, workers: int = 4, total_ops: int = 10000) -> BenchmarkResult:
        """ë™ì‹œ ê²€ì¦ ì„±ëŠ¥ ì¸¡ì •"""
        test_names = self.generate_test_names(total_ops)
        ops_per_worker = total_ops // workers
        
        def worker_task(names_chunk):
            """ì›Œì»¤ íƒœìŠ¤í¬"""
            engine = get_naming_engine()
            times = []
            
            for entity_type, name in names_chunk:
                start = time.perf_counter()
                engine.validate(entity_type, name)
                elapsed = time.perf_counter() - start
                times.append(elapsed)
            
            return times
        
        # ì‘ì—… ë¶„í• 
        chunks = []
        for i in range(workers):
            start_idx = i * ops_per_worker
            end_idx = start_idx + ops_per_worker if i < workers - 1 else total_ops
            chunks.append(test_names[start_idx:end_idx])
        
        gc.collect()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        
        # ë™ì‹œ ì‹¤í–‰
        start_time = time.perf_counter()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [executor.submit(worker_task, chunk) for chunk in chunks]
            all_times = []
            
            for future in concurrent.futures.as_completed(futures):
                all_times.extend(future.result())
        
        total_time = time.perf_counter() - start_time
        
        end_memory = self.process.memory_info().rss / 1024 / 1024
        memory_used = end_memory - start_memory
        
        return BenchmarkResult(
            name=f"Concurrent Validation ({workers} workers)",
            total_operations=total_ops,
            total_time=total_time,
            avg_time=statistics.mean(all_times),
            min_time=min(all_times),
            max_time=max(all_times),
            std_dev=statistics.stdev(all_times),
            ops_per_second=total_ops / total_time,
            memory_used_mb=memory_used
        )
    
    def benchmark_pattern_complexity(self) -> Dict[str, BenchmarkResult]:
        """íŒ¨í„´ ë³µì¡ë„ë³„ ì„±ëŠ¥ ì¸¡ì •"""
        results = {}
        
        # ê°„ë‹¨í•œ íŒ¨í„´
        simple_names = ["Product", "User", "Order"] * 100
        
        # ë³µì¡í•œ íŒ¨í„´ (ì•½ì–´, ìˆ«ì í¬í•¨)
        complex_names = ["HTTPServer", "OAuth2Token", "APIv3Client"] * 100
        
        # ë§¤ìš° ë³µì¡í•œ íŒ¨í„´
        very_complex_names = ["HTTP2ServerErrorHandler", "OAuth2TokenProviderFactory", "XMLHttpRequestManagerV2"] * 100
        
        test_sets = [
            ("Simple Patterns", simple_names),
            ("Complex Patterns", complex_names),
            ("Very Complex Patterns", very_complex_names),
        ]
        
        for name, test_names in test_sets:
            times = []
            
            gc.collect()
            start_memory = self.process.memory_info().rss / 1024 / 1024
            
            total_start = time.perf_counter()
            
            for test_name in test_names:
                start = time.perf_counter()
                self.engine.validate(EntityType.OBJECT_TYPE, test_name)
                elapsed = time.perf_counter() - start
                times.append(elapsed)
            
            total_time = time.perf_counter() - total_start
            
            end_memory = self.process.memory_info().rss / 1024 / 1024
            memory_used = end_memory - start_memory
            
            results[name] = BenchmarkResult(
                name=name,
                total_operations=len(test_names),
                total_time=total_time,
                avg_time=statistics.mean(times),
                min_time=min(times),
                max_time=max(times),
                std_dev=statistics.stdev(times),
                ops_per_second=len(test_names) / total_time,
                memory_used_mb=memory_used
            )
        
        return results
    
    def benchmark_auto_fix_performance(self, iterations: int = 1000) -> BenchmarkResult:
        """ìë™ ìˆ˜ì • ì„±ëŠ¥ ì¸¡ì •"""
        # ìˆ˜ì •ì´ í•„ìš”í•œ ì´ë¦„ë“¤
        invalid_names = [
            ("product_manager", EntityType.OBJECT_TYPE),
            ("FirstName", EntityType.PROPERTY),
            ("product", EntityType.LINK_TYPE),
            ("HTTPClient", EntityType.ACTION_TYPE),
        ] * (iterations // 4)
        
        times = []
        
        gc.collect()
        start_memory = self.process.memory_info().rss / 1024 / 1024
        
        total_start = time.perf_counter()
        
        for name, entity_type in invalid_names:
            start = time.perf_counter()
            self.engine.auto_fix(entity_type, name)
            elapsed = time.perf_counter() - start
            times.append(elapsed)
        
        total_time = time.perf_counter() - total_start
        
        end_memory = self.process.memory_info().rss / 1024 / 1024
        memory_used = end_memory - start_memory
        
        return BenchmarkResult(
            name="Auto-fix Performance",
            total_operations=len(invalid_names),
            total_time=total_time,
            avg_time=statistics.mean(times),
            min_time=min(times),
            max_time=max(times),
            std_dev=statistics.stdev(times),
            ops_per_second=len(invalid_names) / total_time,
            memory_used_mb=memory_used
        )


def run_all_benchmarks():
    """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    print("ğŸš€ Starting Naming Convention Benchmarks...")
    print("=" * 60)
    
    benchmark = NamingBenchmark()
    results = {}
    
    # 1. ë‹¨ì¼ ê²€ì¦ ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ“Š Running single validation benchmark...")
    results['single'] = benchmark.benchmark_single_validation(1000)
    print(results['single'])
    
    # 2. 10K ì—”í‹°í‹° ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ“Š Running 10K entities benchmark...")
    results['10k'] = benchmark.benchmark_10k_entities()
    print(results['10k'])
    
    # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
    print("\nğŸ“Š Profiling memory usage...")
    memory_profile = benchmark.benchmark_memory_usage()
    print("\n=== Memory Usage Profile ===")
    for key, value in memory_profile.items():
        print(f"{key}: {value:.2f} MB")
    
    # 4. ë™ì‹œì„± ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ“Š Running concurrent validation benchmarks...")
    for workers in [1, 2, 4, 8]:
        results[f'concurrent_{workers}'] = benchmark.benchmark_concurrent_validation(workers, 8000)
        print(results[f'concurrent_{workers}'])
    
    # 5. íŒ¨í„´ ë³µì¡ë„ ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ“Š Running pattern complexity benchmarks...")
    pattern_results = benchmark.benchmark_pattern_complexity()
    for name, result in pattern_results.items():
        print(result)
    
    # 6. ìë™ ìˆ˜ì • ì„±ëŠ¥
    print("\nğŸ“Š Running auto-fix performance benchmark...")
    results['autofix'] = benchmark.benchmark_auto_fix_performance(1000)
    print(results['autofix'])
    
    # ê²°ê³¼ ì €ì¥
    summary = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'results': {
            name: {
                'total_operations': r.total_operations,
                'total_time': r.total_time,
                'avg_time_ms': r.avg_time * 1000,
                'ops_per_second': r.ops_per_second,
                'memory_used_mb': r.memory_used_mb
            }
            for name, r in results.items()
        },
        'memory_profile': memory_profile
    }
    
    with open('benchmark_results.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print("\nâœ… Benchmark completed. Results saved to benchmark_results.json")


if __name__ == "__main__":
    run_all_benchmarks()