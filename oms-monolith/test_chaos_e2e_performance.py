#!/usr/bin/env python3
"""
OMS ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸
ê·¹í•œ ìƒí™©ì—ì„œì˜ ì‹œìŠ¤í…œ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ ê²€ì¦
"""
import asyncio
import json
import sys
import os
from datetime import datetime
import httpx
import nats
import random
import string
import statistics
import psutil
import time
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
import aiofiles

sys.path.append('/Users/sihyun/Desktop/ARRAKIS/SPICE/oms-monolith')

from database.simple_terminus_client import SimpleTerminusDBClient

import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ChaosE2EPerformanceTest:
    """ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.base_url = "http://localhost:8002"
        self.nats_url = "nats://localhost:4222"
        self.metrics = {
            "api_latencies": [],
            "event_latencies": [],
            "db_latencies": [],
            "memory_usage": [],
            "cpu_usage": [],
            "errors": [],
            "successful_operations": 0,
            "failed_operations": 0
        }
        self.start_time = None
        self.event_counter = 0
        self.event_timestamps = {}  # ì´ë²¤íŠ¸ ë°œí–‰ ì‹œê°„ ì¶”ì 
        
    async def setup(self):
        """í™˜ê²½ ì„¤ì •"""
        logger.info("ğŸš€ ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •")
        
        # NATS ì—°ê²°
        self.nc = await nats.connect(self.nats_url)
        
        # ì´ë²¤íŠ¸ ìˆ˜ì‹  í•¸ë“¤ëŸ¬ (ì§€ì—° ì‹œê°„ ì¸¡ì •ìš©)
        async def event_handler(msg):
            try:
                data = json.loads(msg.data.decode())
                event_id = data.get('id')
                if event_id and event_id in self.event_timestamps:
                    # ì´ë²¤íŠ¸ ì§€ì—° ì‹œê°„ ê³„ì‚°
                    latency = (datetime.now().timestamp() - self.event_timestamps[event_id]) * 1000
                    self.metrics["event_latencies"].append(latency)
                    del self.event_timestamps[event_id]  # ë©”ëª¨ë¦¬ ì •ë¦¬
            except:
                pass
                
        await self.nc.subscribe("oms.>", cb=event_handler)
        await self.nc.subscribe("com.>", cb=event_handler)
        
        # DB ì—°ê²°
        self.db = SimpleTerminusDBClient(
            endpoint="http://localhost:6363",
            username="admin",
            password="root",
            database="oms"
        )
        await self.db.connect()
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ í’€
        self.http_clients = [httpx.AsyncClient(timeout=30.0) for _ in range(10)]
        
        # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.monitoring_task = asyncio.create_task(self.monitor_system_resources())
        
        self.start_time = datetime.now()
        logger.info("âœ… í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
        
    async def monitor_system_resources(self):
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                # CPU ì‚¬ìš©ë¥ 
                cpu_percent = psutil.cpu_percent(interval=1)
                self.metrics["cpu_usage"].append(cpu_percent)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                memory = psutil.virtual_memory()
                self.metrics["memory_usage"].append(memory.percent)
                
                await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì¸¡ì •
            except:
                break
                
    async def chaos_test_1_extreme_load(self):
        """Chaos Test 1: ê·¹í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ’¥ Chaos Test 1: ê·¹í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸")
        logger.info("="*80)
        
        results = {
            "total_requests": 0,
            "successful": 0,
            "failed": 0,
            "avg_latency": 0,
            "max_latency": 0,
            "min_latency": float('inf'),
            "p95_latency": 0,
            "p99_latency": 0
        }
        
        # 1000ê°œì˜ ë™ì‹œ ìš”ì²­
        logger.info("\nğŸ”¥ 1000ê°œ ë™ì‹œ API ìš”ì²­ ì‹œì‘...")
        
        async def make_request(client_idx, req_idx):
            client = self.http_clients[client_idx % len(self.http_clients)]
            start = time.time()
            
            try:
                # ëœë¤ ì‘ì—… ì„ íƒ
                operation = random.choice(['create', 'read', 'update'])
                
                if operation == 'create':
                    response = await client.post(
                        f"{self.base_url}/api/v1/schemas/main/object-types",
                        json={
                            "name": f"ChaosType_{req_idx}_{random.randint(1000,9999)}",
                            "displayName": f"ì¹´ì˜¤ìŠ¤ íƒ€ì… {req_idx}",
                            "description": "ê·¹í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸"
                        },
                        headers={"Authorization": f"Bearer chaos-user-{req_idx}"}
                    )
                elif operation == 'read':
                    response = await client.get(
                        f"{self.base_url}/api/v1/schemas/main/object-types",
                        headers={"Authorization": f"Bearer chaos-user-{req_idx}"}
                    )
                else:  # update
                    response = await client.get(
                        f"{self.base_url}/health"
                    )
                    
                latency = (time.time() - start) * 1000  # ms
                
                if response.status_code in [200, 201]:
                    self.metrics["api_latencies"].append(latency)
                    return True, latency
                else:
                    self.metrics["errors"].append({
                        "type": "http_error",
                        "status": response.status_code,
                        "operation": operation
                    })
                    return False, latency
                    
            except Exception as e:
                latency = (time.time() - start) * 1000
                self.metrics["errors"].append({
                    "type": "exception",
                    "error": str(e),
                    "operation": operation
                })
                return False, latency
                
        # 1000ê°œ ìš”ì²­ì„ 100ê°œì”© ë°°ì¹˜ë¡œ ì‹¤í–‰
        batch_size = 100
        total_requests = 1000
        
        for batch in range(0, total_requests, batch_size):
            tasks = []
            for i in range(batch_size):
                if batch + i < total_requests:
                    tasks.append(make_request(i, batch + i))
                    
            batch_results = await asyncio.gather(*tasks)
            
            for success, latency in batch_results:
                results["total_requests"] += 1
                if success:
                    results["successful"] += 1
                else:
                    results["failed"] += 1
                    
                if latency < results["min_latency"]:
                    results["min_latency"] = latency
                if latency > results["max_latency"]:
                    results["max_latency"] = latency
                    
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸°
            await asyncio.sleep(0.1)
            
        # í†µê³„ ê³„ì‚°
        if self.metrics["api_latencies"]:
            results["avg_latency"] = statistics.mean(self.metrics["api_latencies"])
            results["p95_latency"] = statistics.quantiles(self.metrics["api_latencies"], n=20)[18]  # 95th percentile
            results["p99_latency"] = statistics.quantiles(self.metrics["api_latencies"], n=100)[98]  # 99th percentile
            
        logger.info(f"\nğŸ“Š ê·¹í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"   ì´ ìš”ì²­: {results['total_requests']}")
        logger.info(f"   ì„±ê³µ: {results['successful']} ({results['successful']/results['total_requests']*100:.1f}%)")
        logger.info(f"   ì‹¤íŒ¨: {results['failed']}")
        logger.info(f"   í‰ê·  ì§€ì—°: {results['avg_latency']:.2f}ms")
        logger.info(f"   ìµœì†Œ ì§€ì—°: {results['min_latency']:.2f}ms")
        logger.info(f"   ìµœëŒ€ ì§€ì—°: {results['max_latency']:.2f}ms")
        logger.info(f"   P95 ì§€ì—°: {results['p95_latency']:.2f}ms")
        logger.info(f"   P99 ì§€ì—°: {results['p99_latency']:.2f}ms")
        
        return results
        
    async def chaos_test_2_event_storm(self):
        """Chaos Test 2: ì´ë²¤íŠ¸ í­í’"""
        logger.info("\n" + "="*80)
        logger.info("ğŸŒªï¸ Chaos Test 2: ì´ë²¤íŠ¸ í­í’ (10,000 ì´ë²¤íŠ¸)")
        logger.info("="*80)
        
        results = {
            "total_events": 10000,
            "published": 0,
            "received": 0,
            "avg_event_latency": 0,
            "max_event_latency": 0,
            "events_per_second": 0
        }
        
        initial_received = len(self.metrics["event_latencies"])
        start_time = time.time()
        
        # 10,000ê°œ ì´ë²¤íŠ¸ ì—°ì† ë°œí–‰
        logger.info("\nğŸ“¤ 10,000ê°œ ì´ë²¤íŠ¸ ë°œí–‰ ì‹œì‘...")
        
        for i in range(results["total_events"]):
            event_id = f"chaos-event-{i}-{random.randint(1000,9999)}"
            self.event_timestamps[event_id] = datetime.now().timestamp()
            
            event = {
                "specversion": "1.0",
                "type": "com.oms.chaos.storm",
                "source": "/oms/chaos",
                "id": event_id,
                "time": datetime.now().isoformat(),
                "datacontenttype": "application/json",
                "data": {
                    "index": i,
                    "test": "event_storm",
                    "payload": "x" * random.randint(100, 1000)  # ê°€ë³€ í¬ê¸° í˜ì´ë¡œë“œ
                }
            }
            
            subject = f"oms.chaos.storm.{i % 100}"  # 100ê°œì˜ ë‹¤ë¥¸ ì£¼ì œë¡œ ë¶„ì‚°
            
            try:
                await self.nc.publish(subject, json.dumps(event).encode())
                results["published"] += 1
                
                # 100ê°œë§ˆë‹¤ ì§§ì€ ëŒ€ê¸°
                if i % 100 == 0:
                    await asyncio.sleep(0.01)
                    
            except Exception as e:
                self.metrics["errors"].append({
                    "type": "event_publish_error",
                    "error": str(e)
                })
                
        publish_time = time.time() - start_time
        results["events_per_second"] = results["published"] / publish_time
        
        logger.info(f"âœ… ë°œí–‰ ì™„ë£Œ: {results['published']}ê°œ ({publish_time:.2f}ì´ˆ)")
        logger.info(f"ğŸ“Š ë°œí–‰ ì†ë„: {results['events_per_second']:.2f} events/sec")
        
        # ìˆ˜ì‹  ëŒ€ê¸°
        logger.info("\nâ³ ì´ë²¤íŠ¸ ìˆ˜ì‹  ëŒ€ê¸° (5ì´ˆ)...")
        await asyncio.sleep(5)
        
        results["received"] = len(self.metrics["event_latencies"]) - initial_received
        
        # ì´ë²¤íŠ¸ ì§€ì—° í†µê³„
        if self.metrics["event_latencies"]:
            recent_latencies = self.metrics["event_latencies"][initial_received:]
            if recent_latencies:
                results["avg_event_latency"] = statistics.mean(recent_latencies)
                results["max_event_latency"] = max(recent_latencies)
                
        logger.info(f"\nğŸ“Š ì´ë²¤íŠ¸ í­í’ ê²°ê³¼:")
        logger.info(f"   ë°œí–‰: {results['published']}")
        logger.info(f"   ìˆ˜ì‹ : {results['received']} ({results['received']/results['published']*100:.1f}%)")
        logger.info(f"   í‰ê·  ì§€ì—°: {results['avg_event_latency']:.2f}ms")
        logger.info(f"   ìµœëŒ€ ì§€ì—°: {results['max_event_latency']:.2f}ms")
        
        return results
        
    async def chaos_test_3_memory_pressure(self):
        """Chaos Test 3: ë©”ëª¨ë¦¬ ì••ë°• í…ŒìŠ¤íŠ¸"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ’¾ Chaos Test 3: ë©”ëª¨ë¦¬ ì••ë°• í…ŒìŠ¤íŠ¸")
        logger.info("="*80)
        
        results = {
            "large_objects_created": 0,
            "memory_before": 0,
            "memory_peak": 0,
            "memory_after": 0,
            "gc_collections": 0
        }
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_before = psutil.virtual_memory().percent
        results["memory_before"] = memory_before
        
        logger.info(f"\nğŸ’¾ ì´ˆê¸° ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory_before:.1f}%")
        
        # ëŒ€ìš©ëŸ‰ ê°ì²´ ìƒì„±
        logger.info("\nğŸ”¥ ëŒ€ìš©ëŸ‰ ë©”íƒ€ë°ì´í„° ê°ì²´ ìƒì„±...")
        
        large_objects = []
        for i in range(100):
            # í° ì„¤ëª…ê³¼ ë§ì€ ì†ì„±ì„ ê°€ì§„ ê°ì²´
            large_object = {
                "name": f"LargeObject_{i}_{random.randint(1000,9999)}",
                "displayName": f"ëŒ€ìš©ëŸ‰ ê°ì²´ {i}",
                "description": "x" * 10000,  # 10KB ì„¤ëª…
                "metadata": {
                    f"field_{j}": f"value_{j}" * 100 
                    for j in range(50)  # 50ê°œ í•„ë“œ
                },
                "tags": [f"tag_{k}" for k in range(100)]  # 100ê°œ íƒœê·¸
            }
            
            try:
                response = await self.http_clients[0].post(
                    f"{self.base_url}/api/v1/schemas/main/object-types",
                    json=large_object,
                    headers={"Authorization": "Bearer memory-test"}
                )
                
                if response.status_code in [200, 201]:
                    results["large_objects_created"] += 1
                    large_objects.append(large_object)
                    
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
                current_memory = psutil.virtual_memory().percent
                if current_memory > results["memory_peak"]:
                    results["memory_peak"] = current_memory
                    
            except Exception as e:
                self.metrics["errors"].append({
                    "type": "memory_test_error",
                    "error": str(e)
                })
                
        # ë™ì‹œì— ë§ì€ ì¡°íšŒ ìš”ì²­
        logger.info("\nğŸ”¥ ëŒ€ëŸ‰ ë™ì‹œ ì¡°íšŒ ìš”ì²­...")
        
        async def bulk_read():
            tasks = []
            for i in range(50):
                task = self.http_clients[i % len(self.http_clients)].get(
                    f"{self.base_url}/api/v1/schemas/main/object-types",
                    headers={"Authorization": f"Bearer reader-{i}"}
                )
                tasks.append(task)
                
            await asyncio.gather(*tasks, return_exceptions=True)
            
        await bulk_read()
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        await asyncio.sleep(2)
        results["memory_after"] = psutil.virtual_memory().percent
        
        logger.info(f"\nğŸ“Š ë©”ëª¨ë¦¬ ì••ë°• í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"   ëŒ€ìš©ëŸ‰ ê°ì²´ ìƒì„±: {results['large_objects_created']}")
        logger.info(f"   ì´ˆê¸° ë©”ëª¨ë¦¬: {results['memory_before']:.1f}%")
        logger.info(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {results['memory_peak']:.1f}%")
        logger.info(f"   ìµœì¢… ë©”ëª¨ë¦¬: {results['memory_after']:.1f}%")
        logger.info(f"   ë©”ëª¨ë¦¬ ì¦ê°€: {results['memory_peak'] - results['memory_before']:.1f}%")
        
        return results
        
    async def chaos_test_4_connection_chaos(self):
        """Chaos Test 4: ì—°ê²° ì¹´ì˜¤ìŠ¤"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ”Œ Chaos Test 4: ì—°ê²° ì¹´ì˜¤ìŠ¤ (ì—°ê²°/í•´ì œ ë°˜ë³µ)")
        logger.info("="*80)
        
        results = {
            "connection_attempts": 0,
            "successful_connections": 0,
            "failed_connections": 0,
            "reconnection_time": [],
            "operations_during_chaos": 0
        }
        
        # ì—°ê²° ìƒì„±/í•´ì œ ë°˜ë³µ
        logger.info("\nğŸ”¥ 500ê°œ ì—°ê²° ìƒì„±/í•´ì œ ë°˜ë³µ...")
        
        async def connection_chaos():
            for i in range(500):
                try:
                    # ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    client = httpx.AsyncClient(timeout=5.0)
                    results["connection_attempts"] += 1
                    
                    # ì¦‰ì‹œ ìš”ì²­
                    response = await client.get(f"{self.base_url}/health")
                    
                    if response.status_code == 200:
                        results["successful_connections"] += 1
                        
                    # ëœë¤í•˜ê²Œ ì—°ê²° ìœ ì§€ ë˜ëŠ” ì¦‰ì‹œ ì¢…ë£Œ
                    if random.random() > 0.5:
                        await asyncio.sleep(random.uniform(0.01, 0.1))
                        
                    await client.aclose()
                    
                except Exception as e:
                    results["failed_connections"] += 1
                    self.metrics["errors"].append({
                        "type": "connection_error",
                        "error": str(e)
                    })
                    
                # 10ê°œë§ˆë‹¤ ì§§ì€ ëŒ€ê¸°
                if i % 10 == 0:
                    await asyncio.sleep(0.01)
                    
        # ì—°ê²° ì¹´ì˜¤ìŠ¤ì™€ ë™ì‹œì— ì •ìƒ ì‘ì—… ìˆ˜í–‰
        async def normal_operations():
            while results["connection_attempts"] < 500:
                try:
                    response = await self.http_clients[0].get(
                        f"{self.base_url}/api/v1/schemas/main/object-types",
                        headers={"Authorization": "Bearer normal-user"}
                    )
                    
                    if response.status_code == 200:
                        results["operations_during_chaos"] += 1
                        
                except:
                    pass
                    
                await asyncio.sleep(0.1)
                
        # ë™ì‹œ ì‹¤í–‰
        await asyncio.gather(
            connection_chaos(),
            normal_operations()
        )
        
        logger.info(f"\nğŸ“Š ì—°ê²° ì¹´ì˜¤ìŠ¤ ê²°ê³¼:")
        logger.info(f"   ì—°ê²° ì‹œë„: {results['connection_attempts']}")
        logger.info(f"   ì„±ê³µ: {results['successful_connections']} ({results['successful_connections']/results['connection_attempts']*100:.1f}%)")
        logger.info(f"   ì‹¤íŒ¨: {results['failed_connections']}")
        logger.info(f"   ì¹´ì˜¤ìŠ¤ ì¤‘ ì •ìƒ ì‘ì—…: {results['operations_during_chaos']}")
        
        return results
        
    async def chaos_test_5_cascade_failure(self):
        """Chaos Test 5: ì—°ì‡„ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜"""
        logger.info("\n" + "="*80)
        logger.info("â›“ï¸ Chaos Test 5: ì—°ì‡„ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜")
        logger.info("="*80)
        
        results = {
            "db_operations": {"success": 0, "failed": 0},
            "api_operations": {"success": 0, "failed": 0},
            "event_operations": {"success": 0, "failed": 0},
            "recovery_time": 0,
            "degraded_performance": False
        }
        
        # Phase 1: ì •ìƒ ì‘ë™ í™•ì¸
        logger.info("\n1ï¸âƒ£ Phase 1: ì •ìƒ ì‘ë™ ê¸°ì¤€ì„  ì¸¡ì •...")
        
        baseline_start = time.time()
        
        # ì •ìƒ ì‘ë™ í…ŒìŠ¤íŠ¸
        for i in range(10):
            try:
                # API í˜¸ì¶œ
                response = await self.http_clients[0].get(f"{self.base_url}/health")
                if response.status_code == 200:
                    results["api_operations"]["success"] += 1
                    
                # ì´ë²¤íŠ¸ ë°œí–‰
                await self.nc.publish("oms.test", b"test")
                results["event_operations"]["success"] += 1
                
            except:
                pass
                
        baseline_time = time.time() - baseline_start
        
        # Phase 2: ë¶€í•˜ ì¦ê°€ë¡œ ì„±ëŠ¥ ì €í•˜ ìœ ë„
        logger.info("\n2ï¸âƒ£ Phase 2: ê·¹ì‹¬í•œ ë¶€í•˜ë¡œ ì„±ëŠ¥ ì €í•˜ ìœ ë„...")
        
        async def heavy_load():
            tasks = []
            for i in range(200):
                task = self.http_clients[i % len(self.http_clients)].post(
                    f"{self.base_url}/api/v1/schemas/main/object-types",
                    json={
                        "name": f"LoadTest_{i}_{random.randint(1000,9999)}",
                        "displayName": f"ë¶€í•˜ í…ŒìŠ¤íŠ¸ {i}",
                        "description": "x" * 5000
                    },
                    headers={"Authorization": f"Bearer load-{i}"},
                    timeout=2.0
                )
                tasks.append(task)
                
            results_heavy = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results_heavy:
                if isinstance(result, Exception):
                    results["api_operations"]["failed"] += 1
                else:
                    results["api_operations"]["success"] += 1
                    
        await heavy_load()
        
        # Phase 3: ë³µêµ¬ ì‹œê°„ ì¸¡ì •
        logger.info("\n3ï¸âƒ£ Phase 3: ì‹œìŠ¤í…œ ë³µêµ¬ ì‹œê°„ ì¸¡ì •...")
        
        recovery_start = time.time()
        recovered = False
        
        while not recovered and (time.time() - recovery_start) < 30:  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
            try:
                response = await self.http_clients[0].get(
                    f"{self.base_url}/health",
                    timeout=1.0
                )
                
                if response.status_code == 200:
                    # ì‘ë‹µ ì‹œê°„ì´ ì •ìƒ ìˆ˜ì¤€ìœ¼ë¡œ ëŒì•„ì™”ëŠ”ì§€ í™•ì¸
                    test_start = time.time()
                    await self.http_clients[0].get(f"{self.base_url}/health")
                    test_time = time.time() - test_start
                    
                    if test_time < baseline_time * 2:  # ê¸°ì¤€ì„ ì˜ 2ë°° ì´ë‚´
                        recovered = True
                        results["recovery_time"] = time.time() - recovery_start
                        
            except:
                pass
                
            await asyncio.sleep(0.5)
            
        logger.info(f"\nğŸ“Š ì—°ì‡„ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:")
        logger.info(f"   API ì„±ê³µ/ì‹¤íŒ¨: {results['api_operations']['success']}/{results['api_operations']['failed']}")
        logger.info(f"   ì´ë²¤íŠ¸ ì‘ì—…: {results['event_operations']['success']}")
        logger.info(f"   ë³µêµ¬ ì‹œê°„: {results['recovery_time']:.2f}ì´ˆ")
        logger.info(f"   ì‹œìŠ¤í…œ ë³µêµ¬: {'âœ… ì„±ê³µ' if recovered else 'âŒ ì‹¤íŒ¨'}")
        
        return results
        
    async def generate_performance_report(self, test_results):
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìµœì¢… ë³´ê³ ì„œ")
        logger.info("="*80)
        
        total_time = (datetime.now() - self.start_time).total_seconds()
        
        # 1. ì „ì²´ ì„±ëŠ¥ ì§€í‘œ
        logger.info("\n### 1. ì „ì²´ ì„±ëŠ¥ ì§€í‘œ")
        logger.info(f"â±ï¸ ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"âœ… ì„±ê³µí•œ ì‘ì—…: {self.metrics['successful_operations']}")
        logger.info(f"âŒ ì‹¤íŒ¨í•œ ì‘ì—…: {self.metrics['failed_operations']}")
        logger.info(f"ğŸ“ ì´ ì˜¤ë¥˜ ìˆ˜: {len(self.metrics['errors'])}")
        
        # 2. API ì„±ëŠ¥
        logger.info("\n### 2. API ì„±ëŠ¥ ë¶„ì„")
        if self.metrics["api_latencies"]:
            logger.info(f"ğŸ“Š API ìš”ì²­ ìˆ˜: {len(self.metrics['api_latencies'])}")
            logger.info(f"â±ï¸ í‰ê·  ì‘ë‹µ ì‹œê°„: {statistics.mean(self.metrics['api_latencies']):.2f}ms")
            logger.info(f"â±ï¸ ì¤‘ê°„ê°’: {statistics.median(self.metrics['api_latencies']):.2f}ms")
            logger.info(f"â±ï¸ P95: {statistics.quantiles(self.metrics['api_latencies'], n=20)[18]:.2f}ms")
            logger.info(f"â±ï¸ P99: {statistics.quantiles(self.metrics['api_latencies'], n=100)[98]:.2f}ms")
            
        # 3. ì´ë²¤íŠ¸ ì„±ëŠ¥
        logger.info("\n### 3. ì´ë²¤íŠ¸ ì²˜ë¦¬ ì„±ëŠ¥")
        if self.metrics["event_latencies"]:
            logger.info(f"ğŸ“Š ì²˜ë¦¬ëœ ì´ë²¤íŠ¸: {len(self.metrics['event_latencies'])}")
            logger.info(f"â±ï¸ í‰ê·  ì´ë²¤íŠ¸ ì§€ì—°: {statistics.mean(self.metrics['event_latencies']):.2f}ms")
            logger.info(f"â±ï¸ ìµœëŒ€ ì§€ì—°: {max(self.metrics['event_latencies']):.2f}ms")
            
        # 4. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
        logger.info("\n### 4. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©")
        if self.metrics["cpu_usage"]:
            logger.info(f"ğŸ–¥ï¸ í‰ê·  CPU ì‚¬ìš©ë¥ : {statistics.mean(self.metrics['cpu_usage']):.1f}%")
            logger.info(f"ğŸ–¥ï¸ ìµœëŒ€ CPU ì‚¬ìš©ë¥ : {max(self.metrics['cpu_usage']):.1f}%")
            
        if self.metrics["memory_usage"]:
            logger.info(f"ğŸ’¾ í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {statistics.mean(self.metrics['memory_usage']):.1f}%")
            logger.info(f"ğŸ’¾ ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {max(self.metrics['memory_usage']):.1f}%")
            
        # 5. ì˜¤ë¥˜ ë¶„ì„
        logger.info("\n### 5. ì˜¤ë¥˜ ë¶„ì„")
        error_types = {}
        for error in self.metrics["errors"]:
            error_type = error.get("type", "unknown")
            error_types[error_type] = error_types.get(error_type, 0) + 1
            
        for error_type, count in error_types.items():
            logger.info(f"   {error_type}: {count}ê°œ")
            
        # 6. ê·¹í•œ ìƒí™© í‰ê°€
        logger.info("\n### 6. ê·¹í•œ ìƒí™© ì²˜ë¦¬ ëŠ¥ë ¥")
        
        # ì ìˆ˜ ê³„ì‚°
        score = 0
        total_score = 5
        
        # ê·¹í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸
        if test_results["extreme_load"]["successful"] > 800:  # 80% ì´ìƒ
            score += 1
            logger.info("âœ… ê·¹í•œ ë¶€í•˜: 1000ê°œ ë™ì‹œ ìš”ì²­ ì¤‘ 80% ì´ìƒ ì„±ê³µ")
        else:
            logger.info("âŒ ê·¹í•œ ë¶€í•˜: ì„±ëŠ¥ ê°œì„  í•„ìš”")
            
        # ì´ë²¤íŠ¸ í­í’
        if test_results["event_storm"]["received"] > 8000:  # 80% ì´ìƒ
            score += 1
            logger.info("âœ… ì´ë²¤íŠ¸ í­í’: 10,000ê°œ ì´ë²¤íŠ¸ ì¤‘ 80% ì´ìƒ ì²˜ë¦¬")
        else:
            logger.info("âŒ ì´ë²¤íŠ¸ í­í’: ì´ë²¤íŠ¸ ì²˜ë¦¬ ê°œì„  í•„ìš”")
            
        # ë©”ëª¨ë¦¬ ì••ë°•
        memory_increase = test_results["memory_pressure"]["memory_peak"] - test_results["memory_pressure"]["memory_before"]
        if memory_increase < 20:  # 20% ë¯¸ë§Œ ì¦ê°€
            score += 1
            logger.info("âœ… ë©”ëª¨ë¦¬ ê´€ë¦¬: ì•ˆì •ì ")
        else:
            logger.info("âš ï¸ ë©”ëª¨ë¦¬ ê´€ë¦¬: ì£¼ì˜ í•„ìš”")
            
        # ì—°ê²° ì¹´ì˜¤ìŠ¤
        if test_results["connection_chaos"]["successful_connections"] > 450:  # 90% ì´ìƒ
            score += 1
            logger.info("âœ… ì—°ê²° ì•ˆì •ì„±: ìš°ìˆ˜")
        else:
            logger.info("âŒ ì—°ê²° ì•ˆì •ì„±: ê°œì„  í•„ìš”")
            
        # ë³µêµ¬ ëŠ¥ë ¥
        if test_results["cascade_failure"]["recovery_time"] < 5:  # 5ì´ˆ ì´ë‚´
            score += 1
            logger.info("âœ… ë³µêµ¬ ëŠ¥ë ¥: ë¹ ë¥¸ ë³µêµ¬")
        else:
            logger.info("âš ï¸ ë³µêµ¬ ëŠ¥ë ¥: ê°œì„  ì—¬ì§€ ìˆìŒ")
            
        logger.info(f"\nğŸ† ê·¹í•œ ìƒí™© ì ìˆ˜: {score}/{total_score} ({score/total_score*100:.0f}%)")
        
        # 7. í”„ë¡œë•ì…˜ ì¤€ë¹„ë„
        logger.info("\n### 7. í”„ë¡œë•ì…˜ ì¤€ë¹„ë„ í‰ê°€")
        
        production_ready = score >= 3  # 60% ì´ìƒ
        
        if production_ready:
            logger.info("âœ… í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ")
            logger.info("   - ê·¹í•œ ë¶€í•˜ ì²˜ë¦¬ ê°€ëŠ¥")
            logger.info("   - ì•ˆì •ì ì¸ ì´ë²¤íŠ¸ ì²˜ë¦¬")
            logger.info("   - ì ì ˆí•œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬")
        else:
            logger.info("âš ï¸ í”„ë¡œë•ì…˜ ë°°í¬ ì „ ê°œì„  í•„ìš”")
            logger.info("   ê¶Œì¥ ì‚¬í•­:")
            logger.info("   - API ì‘ë‹µ ì‹œê°„ ìµœì í™”")
            logger.info("   - ì´ë²¤íŠ¸ ì²˜ë¦¬ ìš©ëŸ‰ ì¦ëŒ€")
            logger.info("   - ì—°ê²° í’€ ê´€ë¦¬ ê°œì„ ")
            
        # 8. ê¶Œì¥ ì‚¬í•­
        logger.info("\n### 8. ì„±ëŠ¥ ê°œì„  ê¶Œì¥ ì‚¬í•­")
        
        if statistics.mean(self.metrics["api_latencies"]) > 500:
            logger.info("1. API ìºì‹± ë„ì… ê²€í† ")
            
        if len(self.metrics["errors"]) > 100:
            logger.info("2. ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ ê°•í™”")
            
        if max(self.metrics["memory_usage"]) > 80:
            logger.info("3. ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™” í•„ìš”")
            
        logger.info("4. ë¡œë“œ ë°¸ëŸ°ì„œ ë„ì… ê²€í† ")
        logger.info("5. ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹± ìµœì í™”")
        
        # ê²°ê³¼ ì €ì¥
        report_data = {
            "test_time": datetime.now().isoformat(),
            "duration_seconds": total_time,
            "metrics": {
                "api_requests": len(self.metrics["api_latencies"]),
                "events_processed": len(self.metrics["event_latencies"]),
                "errors": len(self.metrics["errors"]),
                "avg_api_latency": statistics.mean(self.metrics["api_latencies"]) if self.metrics["api_latencies"] else 0,
                "avg_event_latency": statistics.mean(self.metrics["event_latencies"]) if self.metrics["event_latencies"] else 0,
            },
            "test_results": test_results,
            "production_ready": production_ready,
            "score": f"{score}/{total_score}"
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        async with aiofiles.open("chaos_e2e_performance_report.json", "w") as f:
            await f.write(json.dumps(report_data, indent=2))
            
        logger.info("\nğŸ’¾ ìƒì„¸ ë³´ê³ ì„œê°€ chaos_e2e_performance_report.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    async def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        self.monitoring_task.cancel()
        await self.nc.close()
        for client in self.http_clients:
            await client.aclose()
        await self.db.disconnect()
        
    async def run_all_tests(self):
        """ëª¨ë“  ì¹´ì˜¤ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        await self.setup()
        
        test_results = {}
        
        # ê° ì¹´ì˜¤ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results["extreme_load"] = await self.chaos_test_1_extreme_load()
        await asyncio.sleep(2)  # í…ŒìŠ¤íŠ¸ ê°„ íœ´ì‹
        
        test_results["event_storm"] = await self.chaos_test_2_event_storm()
        await asyncio.sleep(2)
        
        test_results["memory_pressure"] = await self.chaos_test_3_memory_pressure()
        await asyncio.sleep(2)
        
        test_results["connection_chaos"] = await self.chaos_test_4_connection_chaos()
        await asyncio.sleep(2)
        
        test_results["cascade_failure"] = await self.chaos_test_5_cascade_failure()
        
        # ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        await self.generate_performance_report(test_results)
        
        await self.cleanup()


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    test = ChaosE2EPerformanceTest()
    await test.run_all_tests()


if __name__ == "__main__":
    logger.info("ğŸš€ OMS ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ ê²€ì¦ ì‹œì‘")
    logger.info("ê·¹í•œ ìƒí™©ì—ì„œì˜ ì‹œìŠ¤í…œ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤...")
    asyncio.run(main())