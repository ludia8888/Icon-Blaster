#!/usr/bin/env python3
"""
OMS ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ìƒì„¸ ë¡œê·¸ ë²„ì „)
ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •í•˜ì—¬ ì™„ë£Œ ê°€ëŠ¥í•˜ë„ë¡ í•¨
"""
import asyncio
import json
import sys
import os
from datetime import datetime
import httpx
import nats
import random
import string
import statistics
import psutil
import time
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor
import aiofiles

sys.path.append('/Users/sihyun/Desktop/ARRAKIS/SPICE/oms-monolith')

from database.simple_terminus_client import SimpleTerminusDBClient

import logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chaos_test_detailed.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class DetailedChaosTest:
    """ìƒì„¸ ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.base_url = "http://localhost:8002"
        self.nats_url = "nats://localhost:4222"
        self.metrics = {
            "api_latencies": [],
            "event_latencies": [],
            "errors": [],
            "successful_operations": 0,
            "failed_operations": 0
        }
        self.start_time = None
        self.event_counter = 0
        self.event_timestamps = {}
        
    async def setup(self):
        """í™˜ê²½ ì„¤ì •"""
        logger.info("="*80)
        logger.info("ğŸš€ OMS ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ (ìƒì„¸ ë²„ì „)")
        logger.info("="*80)
        
        # NATS ì—°ê²°
        self.nc = await nats.connect(self.nats_url)
        logger.info("âœ… NATS ì—°ê²° ì„±ê³µ")
        
        # ì´ë²¤íŠ¸ ìˆ˜ì‹  í•¸ë“¤ëŸ¬
        async def event_handler(msg):
            try:
                data = json.loads(msg.data.decode())
                event_id = data.get('id')
                if event_id and event_id in self.event_timestamps:
                    latency = (datetime.now().timestamp() - self.event_timestamps[event_id]) * 1000
                    self.metrics["event_latencies"].append(latency)
                    del self.event_timestamps[event_id]
                    logger.debug(f"ì´ë²¤íŠ¸ ìˆ˜ì‹ : {msg.subject}, ì§€ì—°: {latency:.2f}ms")
            except:
                pass
                
        await self.nc.subscribe("oms.>", cb=event_handler)
        await self.nc.subscribe("com.>", cb=event_handler)
        
        # DB ì—°ê²°
        self.db = SimpleTerminusDBClient(
            endpoint="http://localhost:6363",
            username="admin",
            password="root",
            database="oms"
        )
        await self.db.connect()
        logger.info("âœ… TerminusDB ì—°ê²° ì„±ê³µ")
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ í’€
        self.http_clients = [httpx.AsyncClient(timeout=30.0) for _ in range(5)]
        logger.info("âœ… HTTP í´ë¼ì´ì–¸íŠ¸ í’€ ìƒì„± (5ê°œ)")
        
        self.start_time = datetime.now()
        logger.info(f"í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
    async def chaos_test_1_burst_load(self):
        """Test 1: ìˆœê°„ í­ë°œì  ë¶€í•˜ (500ê°œ)"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ’¥ Chaos Test 1: ìˆœê°„ í­ë°œì  ë¶€í•˜ í…ŒìŠ¤íŠ¸ (500ê°œ ë™ì‹œ ìš”ì²­)")
        logger.info("="*80)
        
        results = {
            "total_requests": 500,
            "successful": 0,
            "failed": 0,
            "latencies": [],
            "errors_by_type": {}
        }
        
        logger.info("ğŸ”¥ 500ê°œ ë™ì‹œ API ìš”ì²­ ì‹œì‘...")
        
        async def make_request(client_idx, req_idx):
            client = self.http_clients[client_idx % len(self.http_clients)]
            start = time.time()
            operation = random.choice(['create', 'read', 'update'])
            
            try:
                if operation == 'create':
                    response = await client.post(
                        f"{self.base_url}/api/v1/schemas/main/object-types",
                        json={
                            "name": f"Burst_{req_idx}_{random.randint(1000,9999)}",
                            "displayName": f"ë¶€í•˜ í…ŒìŠ¤íŠ¸ {req_idx}",
                            "description": "ê·¹í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸"
                        },
                        headers={"Authorization": f"Bearer burst-{req_idx}"}
                    )
                elif operation == 'read':
                    response = await client.get(
                        f"{self.base_url}/api/v1/schemas/main/object-types",
                        headers={"Authorization": f"Bearer burst-{req_idx}"}
                    )
                else:  # update
                    response = await client.get(f"{self.base_url}/health")
                    
                latency = (time.time() - start) * 1000
                
                if response.status_code in [200, 201]:
                    return True, latency, operation
                else:
                    error_type = f"{operation}_{response.status_code}"
                    return False, latency, error_type
                    
            except Exception as e:
                latency = (time.time() - start) * 1000
                error_type = f"{operation}_exception_{type(e).__name__}"
                return False, latency, error_type
                
        # 50ê°œì”© ë°°ì¹˜ë¡œ ì‹¤í–‰í•˜ì—¬ ì§„í–‰ ìƒí™© í‘œì‹œ
        batch_size = 50
        total_batches = results["total_requests"] // batch_size
        
        for batch_num in range(total_batches):
            start_idx = batch_num * batch_size
            logger.info(f"  ë°°ì¹˜ {batch_num + 1}/{total_batches} ì‹¤í–‰ ì¤‘... ({start_idx}-{start_idx + batch_size})")
            
            tasks = []
            for i in range(batch_size):
                tasks.append(make_request(i, start_idx + i))
                
            batch_start = time.time()
            batch_results = await asyncio.gather(*tasks)
            batch_time = time.time() - batch_start
            
            batch_success = 0
            for success, latency, info in batch_results:
                results["latencies"].append(latency)
                if success:
                    results["successful"] += 1
                    batch_success += 1
                else:
                    results["failed"] += 1
                    results["errors_by_type"][info] = results["errors_by_type"].get(info, 0) + 1
                    
            logger.info(f"    ë°°ì¹˜ ì™„ë£Œ: ì„±ê³µ {batch_success}/{batch_size}, í‰ê·  ì§€ì—°: {statistics.mean([r[1] for r in batch_results]):.2f}ms, ì†Œìš”ì‹œê°„: {batch_time:.2f}ì´ˆ")
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸°
            await asyncio.sleep(0.1)
            
        # í†µê³„ ê³„ì‚° ë° ë¡œê¹…
        logger.info("\nğŸ“Š ìˆœê°„ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"  ì´ ìš”ì²­: {results['total_requests']}")
        logger.info(f"  ì„±ê³µ: {results['successful']} ({results['successful']/results['total_requests']*100:.1f}%)")
        logger.info(f"  ì‹¤íŒ¨: {results['failed']} ({results['failed']/results['total_requests']*100:.1f}%)")
        
        if results["latencies"]:
            logger.info(f"  í‰ê·  ì§€ì—°: {statistics.mean(results['latencies']):.2f}ms")
            logger.info(f"  ìµœì†Œ ì§€ì—°: {min(results['latencies']):.2f}ms")
            logger.info(f"  ìµœëŒ€ ì§€ì—°: {max(results['latencies']):.2f}ms")
            logger.info(f"  ì¤‘ê°„ê°’: {statistics.median(results['latencies']):.2f}ms")
            
            if len(results["latencies"]) >= 20:
                logger.info(f"  P95 ì§€ì—°: {statistics.quantiles(results['latencies'], n=20)[18]:.2f}ms")
            if len(results["latencies"]) >= 100:
                logger.info(f"  P99 ì§€ì—°: {statistics.quantiles(results['latencies'], n=100)[98]:.2f}ms")
                
        if results["errors_by_type"]:
            logger.info("\n  ì˜¤ë¥˜ ë¶„ì„:")
            for error_type, count in sorted(results["errors_by_type"].items(), key=lambda x: x[1], reverse=True):
                logger.info(f"    {error_type}: {count}ê°œ")
                
        return results
        
    async def chaos_test_2_event_storm(self):
        """Test 2: ì´ë²¤íŠ¸ í­í’ (5000ê°œ)"""
        logger.info("\n" + "="*80)
        logger.info("ğŸŒªï¸ Chaos Test 2: ì´ë²¤íŠ¸ í­í’ (5,000 ì´ë²¤íŠ¸)")
        logger.info("="*80)
        
        results = {
            "total_events": 5000,
            "published": 0,
            "publish_errors": 0,
            "received": 0,
            "event_latencies": []
        }
        
        initial_received = len(self.metrics["event_latencies"])
        start_time = time.time()
        
        logger.info("ğŸ“¤ 5,000ê°œ ì´ë²¤íŠ¸ ë°œí–‰ ì‹œì‘...")
        
        # 500ê°œì”© ë°°ì¹˜ë¡œ ë°œí–‰
        batch_size = 500
        total_batches = results["total_events"] // batch_size
        
        for batch_num in range(total_batches):
            batch_start = time.time()
            batch_published = 0
            
            logger.info(f"  ë°°ì¹˜ {batch_num + 1}/{total_batches} ë°œí–‰ ì¤‘...")
            
            for i in range(batch_size):
                event_idx = batch_num * batch_size + i
                event_id = f"storm-{event_idx}-{random.randint(1000,9999)}"
                self.event_timestamps[event_id] = datetime.now().timestamp()
                
                event = {
                    "specversion": "1.0",
                    "type": "com.oms.chaos.storm",
                    "source": "/oms/chaos",
                    "id": event_id,
                    "time": datetime.now().isoformat(),
                    "datacontenttype": "application/json",
                    "data": {
                        "index": event_idx,
                        "batch": batch_num,
                        "test": "event_storm",
                        "payload": "x" * random.randint(100, 500)
                    }
                }
                
                subject = f"oms.chaos.storm.{event_idx % 50}"
                
                try:
                    await self.nc.publish(subject, json.dumps(event).encode())
                    results["published"] += 1
                    batch_published += 1
                except Exception as e:
                    results["publish_errors"] += 1
                    logger.debug(f"ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨: {e}")
                    
            batch_time = time.time() - batch_start
            logger.info(f"    ë°°ì¹˜ ì™„ë£Œ: {batch_published}ê°œ ë°œí–‰, ì†Œìš”ì‹œê°„: {batch_time:.2f}ì´ˆ, ì†ë„: {batch_published/batch_time:.2f} events/sec")
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸°
            await asyncio.sleep(0.05)
            
        publish_time = time.time() - start_time
        
        # ìˆ˜ì‹  ëŒ€ê¸°
        logger.info("\nâ³ ì´ë²¤íŠ¸ ìˆ˜ì‹  ëŒ€ê¸° (3ì´ˆ)...")
        await asyncio.sleep(3)
        
        results["received"] = len(self.metrics["event_latencies"]) - initial_received
        results["event_latencies"] = self.metrics["event_latencies"][initial_received:]
        
        # ê²°ê³¼ ë¡œê¹…
        logger.info("\nğŸ“Š ì´ë²¤íŠ¸ í­í’ ê²°ê³¼:")
        logger.info(f"  ë°œí–‰ ì‹œë„: {results['total_events']}")
        logger.info(f"  ë°œí–‰ ì„±ê³µ: {results['published']} ({results['published']/results['total_events']*100:.1f}%)")
        logger.info(f"  ë°œí–‰ ì‹¤íŒ¨: {results['publish_errors']}")
        logger.info(f"  ë°œí–‰ ì†ë„: {results['published']/publish_time:.2f} events/sec")
        logger.info(f"  ìˆ˜ì‹ : {results['received']} ({results['received']/results['published']*100:.1f}% if results['published'] > 0 else 0)")
        
        if results["event_latencies"]:
            logger.info(f"  í‰ê·  ì´ë²¤íŠ¸ ì§€ì—°: {statistics.mean(results['event_latencies']):.2f}ms")
            logger.info(f"  ìµœëŒ€ ì´ë²¤íŠ¸ ì§€ì—°: {max(results['event_latencies']):.2f}ms")
            logger.info(f"  ìµœì†Œ ì´ë²¤íŠ¸ ì§€ì—°: {min(results['event_latencies']):.2f}ms")
            
        return results
        
    async def chaos_test_3_concurrent_chaos(self):
        """Test 3: ë™ì‹œì„± ì¹´ì˜¤ìŠ¤"""
        logger.info("\n" + "="*80)
        logger.info("ğŸŒ€ Chaos Test 3: ë™ì‹œì„± ì¹´ì˜¤ìŠ¤")
        logger.info("="*80)
        
        results = {
            "concurrent_updates": {"attempts": 100, "success": 0, "failed": 0},
            "mixed_operations": {"total": 90, "success": 0, "failed": 0},
            "race_conditions_detected": 0
        }
        
        # 1. ë™ì¼ ê°ì²´ì— ëŒ€í•œ ë™ì‹œ ìˆ˜ì •
        logger.info("\n1ï¸âƒ£ ë™ì¼ ê°ì²´ 100ê°œ ë™ì‹œ ìˆ˜ì • í…ŒìŠ¤íŠ¸...")
        
        # ë¨¼ì € í…ŒìŠ¤íŠ¸ ê°ì²´ ìƒì„±
        test_obj_name = f"ConcurrentTest_{random.randint(1000,9999)}"
        create_resp = await self.http_clients[0].post(
            f"{self.base_url}/api/v1/schemas/main/object-types",
            json={
                "name": test_obj_name,
                "displayName": "ë™ì‹œì„± í…ŒìŠ¤íŠ¸",
                "description": "ì´ˆê¸° ì„¤ëª…"
            },
            headers={"Authorization": "Bearer creator"}
        )
        
        if create_resp.status_code == 200:
            obj_data = create_resp.json()
            obj_id = obj_data.get('id', test_obj_name)
            logger.info(f"  í…ŒìŠ¤íŠ¸ ê°ì²´ ìƒì„± ì™„ë£Œ: {obj_id}")
            
            # 100ê°œ ë™ì‹œ ìˆ˜ì • ì‹œë„
            async def concurrent_update(idx):
                try:
                    resp = await self.http_clients[idx % len(self.http_clients)].put(
                        f"{self.base_url}/api/v1/schemas/main/object-types/{obj_id}",
                        json={"description": f"ë™ì‹œ ìˆ˜ì • #{idx} - {datetime.now().isoformat()}"},
                        headers={"Authorization": f"Bearer updater-{idx}"}
                    )
                    return resp.status_code == 200, resp.status_code
                except Exception as e:
                    return False, str(e)
                    
            logger.info("  100ê°œ ë™ì‹œ ìˆ˜ì • ì‹¤í–‰...")
            update_start = time.time()
            tasks = [concurrent_update(i) for i in range(100)]
            update_results = await asyncio.gather(*tasks)
            update_time = time.time() - update_start
            
            for success, info in update_results:
                if success:
                    results["concurrent_updates"]["success"] += 1
                else:
                    results["concurrent_updates"]["failed"] += 1
                    if isinstance(info, int) and info == 409:  # Conflict
                        results["race_conditions_detected"] += 1
                        
            logger.info(f"  ì™„ë£Œ: ì„±ê³µ {results['concurrent_updates']['success']}/100, ì†Œìš”ì‹œê°„: {update_time:.2f}ì´ˆ")
            logger.info(f"  ê²½ìŸ ìƒíƒœ ê°ì§€: {results['race_conditions_detected']}ê±´")
            
        # 2. í˜¼í•© ì‘ì—… ë™ì‹œ ì‹¤í–‰
        logger.info("\n2ï¸âƒ£ ì´ë²¤íŠ¸ + API + DB í˜¼í•© ì‘ì—… (90ê°œ)...")
        
        async def mixed_chaos():
            tasks = []
            
            # 30ê°œ ì´ë²¤íŠ¸ ë°œí–‰
            for i in range(30):
                event_task = self.nc.publish(
                    "oms.chaos.mixed",
                    json.dumps({
                        "id": f"mixed-{i}",
                        "time": datetime.now().isoformat(),
                        "type": "chaos_test"
                    }).encode()
                )
                tasks.append(("event", event_task))
                
            # 30ê°œ API í˜¸ì¶œ
            for i in range(30):
                api_task = self.http_clients[i % len(self.http_clients)].get(
                    f"{self.base_url}/api/v1/schemas/main/object-types",
                    headers={"Authorization": f"Bearer mixed-{i}"}
                )
                tasks.append(("api", api_task))
                
            # 30ê°œ DB ì§ì ‘ ì¡°íšŒ
            for i in range(30):
                db_task = self.db.client.get(
                    "http://localhost:6363/api/document/admin/oms?type=ObjectType&limit=1",
                    auth=("admin", "root")
                )
                tasks.append(("db", db_task))
                
            # ëª¨ë“  ì‘ì—… ë™ì‹œ ì‹¤í–‰
            logger.info("  90ê°œ í˜¼í•© ì‘ì—… ë™ì‹œ ì‹¤í–‰...")
            mixed_start = time.time()
            
            task_results = await asyncio.gather(*[t[1] for t in tasks], return_exceptions=True)
            mixed_time = time.time() - mixed_start
            
            # ê²°ê³¼ ë¶„ì„
            success_by_type = {"event": 0, "api": 0, "db": 0}
            fail_by_type = {"event": 0, "api": 0, "db": 0}
            
            for i, result in enumerate(task_results):
                task_type = tasks[i][0]
                if not isinstance(result, Exception):
                    results["mixed_operations"]["success"] += 1
                    success_by_type[task_type] += 1
                else:
                    results["mixed_operations"]["failed"] += 1
                    fail_by_type[task_type] += 1
                    
            logger.info(f"  ì™„ë£Œ: ì„±ê³µ {results['mixed_operations']['success']}/90, ì†Œìš”ì‹œê°„: {mixed_time:.2f}ì´ˆ")
            logger.info(f"  ì„±ê³µ ë¶„ì„ - ì´ë²¤íŠ¸: {success_by_type['event']}/30, API: {success_by_type['api']}/30, DB: {success_by_type['db']}/30")
            
        await mixed_chaos()
        
        logger.info("\nğŸ“Š ë™ì‹œì„± ì¹´ì˜¤ìŠ¤ ê²°ê³¼:")
        logger.info(f"  ë™ì¼ ê°ì²´ ìˆ˜ì •: ì„±ê³µ {results['concurrent_updates']['success']}/100")
        logger.info(f"  ê²½ìŸ ìƒíƒœ: {results['race_conditions_detected']}ê±´")
        logger.info(f"  í˜¼í•© ì‘ì—…: ì„±ê³µ {results['mixed_operations']['success']}/90")
        
        return results
        
    async def chaos_test_4_recovery(self):
        """Test 4: ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ”§ Chaos Test 4: ì¥ì•  ë³µêµ¬ ëŠ¥ë ¥")
        logger.info("="*80)
        
        results = {
            "baseline_latency": 0,
            "stressed_latency": 0,
            "recovery_time": 0,
            "recovered": False,
            "stress_operations": {"success": 0, "failed": 0}
        }
        
        # 1. ê¸°ì¤€ì„  ì¸¡ì •
        logger.info("\n1ï¸âƒ£ ì •ìƒ ìƒíƒœ ê¸°ì¤€ì„  ì¸¡ì •...")
        baseline_latencies = []
        
        for i in range(10):
            start = time.time()
            try:
                resp = await self.http_clients[0].get(f"{self.base_url}/health")
                if resp.status_code == 200:
                    latency = (time.time() - start) * 1000
                    baseline_latencies.append(latency)
            except:
                pass
            await asyncio.sleep(0.1)
            
        if baseline_latencies:
            results["baseline_latency"] = statistics.mean(baseline_latencies)
            logger.info(f"  ê¸°ì¤€ ì‘ë‹µì‹œê°„: {results['baseline_latency']:.2f}ms (10íšŒ í‰ê· )")
        
        # 2. ê·¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤ ë¶€í•˜
        logger.info("\n2ï¸âƒ£ ê·¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤ ë¶€í•˜ ê°€ì¤‘...")
        logger.info("  200ê°œ ëŒ€ìš©ëŸ‰ ê°ì²´ ë™ì‹œ ìƒì„± ì‹œë„...")
        
        async def stress_operation(idx):
            try:
                # ëŒ€ìš©ëŸ‰ í˜ì´ë¡œë“œ
                large_description = "x" * 5000  # 5KB
                large_metadata = {f"field_{j}": f"value_{j}" * 50 for j in range(20)}
                
                resp = await self.http_clients[idx % len(self.http_clients)].post(
                    f"{self.base_url}/api/v1/schemas/main/object-types",
                    json={
                        "name": f"Stress_{idx}_{random.randint(1000,9999)}",
                        "displayName": f"ìŠ¤íŠ¸ë ˆìŠ¤ {idx}",
                        "description": large_description,
                        "metadata": large_metadata
                    },
                    headers={"Authorization": f"Bearer stress-{idx}"},
                    timeout=5.0
                )
                return resp.status_code in [200, 201]
            except:
                return False
                
        stress_start = time.time()
        tasks = [stress_operation(i) for i in range(200)]
        stress_results = await asyncio.gather(*tasks, return_exceptions=True)
        stress_time = time.time() - stress_start
        
        for result in stress_results:
            if result is True:
                results["stress_operations"]["success"] += 1
            else:
                results["stress_operations"]["failed"] += 1
                
        logger.info(f"  ìŠ¤íŠ¸ë ˆìŠ¤ ì‘ì—… ì™„ë£Œ: ì„±ê³µ {results['stress_operations']['success']}/200, ì†Œìš”ì‹œê°„: {stress_time:.2f}ì´ˆ")
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ ì§í›„ ìƒíƒœ ì¸¡ì •
        logger.info("\n3ï¸âƒ£ ìŠ¤íŠ¸ë ˆìŠ¤ ì§í›„ ìƒíƒœ ì¸¡ì •...")
        stressed_latencies = []
        
        for i in range(5):
            start = time.time()
            try:
                resp = await self.http_clients[0].get(f"{self.base_url}/health", timeout=3.0)
                latency = (time.time() - start) * 1000
                stressed_latencies.append(latency)
                logger.info(f"  ìŠ¤íŠ¸ë ˆìŠ¤ ìƒíƒœ ì‘ë‹µ: {latency:.2f}ms")
            except asyncio.TimeoutError:
                stressed_latencies.append(3000)  # timeout
                logger.info("  ìŠ¤íŠ¸ë ˆìŠ¤ ìƒíƒœ ì‘ë‹µ: TIMEOUT (3000ms)")
            except:
                stressed_latencies.append(3000)
                
        results["stressed_latency"] = statistics.mean(stressed_latencies)
        logger.info(f"  ìŠ¤íŠ¸ë ˆìŠ¤ ì‘ë‹µì‹œê°„: {results['stressed_latency']:.2f}ms (5íšŒ í‰ê· )")
        
        # 3. ë³µêµ¬ ì‹œê°„ ì¸¡ì •
        logger.info("\n4ï¸âƒ£ ì‹œìŠ¤í…œ ë³µêµ¬ ëª¨ë‹ˆí„°ë§...")
        recovery_start = time.time()
        check_count = 0
        
        while (time.time() - recovery_start) < 30:  # ìµœëŒ€ 30ì´ˆ
            check_count += 1
            try:
                start = time.time()
                resp = await self.http_clients[0].get(f"{self.base_url}/health", timeout=2.0)
                latency = (time.time() - start) * 1000
                
                logger.info(f"  ë³µêµ¬ ì²´í¬ #{check_count}: {latency:.2f}ms")
                
                # ê¸°ì¤€ì„ ì˜ 2ë°° ì´ë‚´ë¡œ ëŒì•„ì˜¤ë©´ ë³µêµ¬
                if latency < results["baseline_latency"] * 2:
                    results["recovery_time"] = time.time() - recovery_start
                    results["recovered"] = True
                    logger.info(f"  âœ… ì‹œìŠ¤í…œ ë³µêµ¬ ì™„ë£Œ! (ë³µêµ¬ ì‹œê°„: {results['recovery_time']:.2f}ì´ˆ)")
                    break
                    
            except:
                logger.info(f"  ë³µêµ¬ ì²´í¬ #{check_count}: ì‹¤íŒ¨")
                
            await asyncio.sleep(1)
            
        if not results["recovered"]:
            logger.info("  âŒ 30ì´ˆ ë‚´ ë³µêµ¬ ì‹¤íŒ¨")
            
        logger.info("\nğŸ“Š ì¥ì•  ë³µêµ¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        logger.info(f"  ê¸°ì¤€ ì‘ë‹µì‹œê°„: {results['baseline_latency']:.2f}ms")
        logger.info(f"  ìŠ¤íŠ¸ë ˆìŠ¤ ì‘ë‹µì‹œê°„: {results['stressed_latency']:.2f}ms ({results['stressed_latency']/results['baseline_latency']:.1f}ë°° ì¦ê°€)")
        logger.info(f"  ë³µêµ¬ ì‹œê°„: {results['recovery_time']:.2f}ì´ˆ")
        logger.info(f"  ë³µêµ¬ ìƒíƒœ: {'âœ… ì„±ê³µ' if results['recovered'] else 'âŒ ì‹¤íŒ¨'}")
        
        return results
        
    async def generate_final_report(self, test_results):
        """ìµœì¢… ì¢…í•© ë³´ê³ ì„œ"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìµœì¢… ë³´ê³ ì„œ")
        logger.info("="*80)
        
        total_time = (datetime.now() - self.start_time).total_seconds()
        
        # 1. í…ŒìŠ¤íŠ¸ ìš”ì•½
        logger.info("\n### 1. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìš”ì•½")
        logger.info(f"â±ï¸ ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ğŸš€ í…ŒìŠ¤íŠ¸ í•­ëª©: 4ê°œ")
        logger.info(f"ğŸ“Š ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­:")
        logger.info(f"   - API ì§€ì—° ì‹œê°„: {len(self.metrics['api_latencies'])}ê°œ")
        logger.info(f"   - ì´ë²¤íŠ¸ ì§€ì—° ì‹œê°„: {len(self.metrics['event_latencies'])}ê°œ")
        logger.info(f"   - ì˜¤ë¥˜ ê¸°ë¡: {len(self.metrics['errors'])}ê°œ")
        
        # 2. ê° í…ŒìŠ¤íŠ¸ ì ìˆ˜
        logger.info("\n### 2. ê°œë³„ í…ŒìŠ¤íŠ¸ í‰ê°€")
        
        scores = {}
        
        # ìˆœê°„ ë¶€í•˜ ì ìˆ˜
        burst = test_results.get("burst_load", {})
        burst_success_rate = burst.get("successful", 0) / burst.get("total_requests", 1) * 100
        scores["burst"] = 1 if burst_success_rate >= 70 else 0
        logger.info(f"\nğŸ’¥ ìˆœê°„ ë¶€í•˜ ì²˜ë¦¬:")
        logger.info(f"   ì„±ê³µë¥ : {burst_success_rate:.1f}%")
        logger.info(f"   ì ìˆ˜: {'âœ… í†µê³¼' if scores['burst'] else 'âŒ ì‹¤íŒ¨'}")
        
        # ì´ë²¤íŠ¸ ì²˜ë¦¬ ì ìˆ˜
        event = test_results.get("event_storm", {})
        event_success_rate = event.get("published", 0) / event.get("total_events", 1) * 100
        event_receive_rate = event.get("received", 0) / event.get("published", 1) * 100 if event.get("published", 0) > 0 else 0
        scores["event"] = 1 if event_success_rate >= 90 and event_receive_rate >= 50 else 0
        logger.info(f"\nğŸŒªï¸ ì´ë²¤íŠ¸ ì²˜ë¦¬:")
        logger.info(f"   ë°œí–‰ ì„±ê³µë¥ : {event_success_rate:.1f}%")
        logger.info(f"   ìˆ˜ì‹ ë¥ : {event_receive_rate:.1f}%")
        logger.info(f"   ì ìˆ˜: {'âœ… í†µê³¼' if scores['event'] else 'âŒ ì‹¤íŒ¨'}")
        
        # ë™ì‹œì„± ì ìˆ˜
        concurrent = test_results.get("concurrent_chaos", {})
        concurrent_success = concurrent.get("concurrent_updates", {}).get("success", 0)
        mixed_success_rate = concurrent.get("mixed_operations", {}).get("success", 0) / concurrent.get("mixed_operations", {}).get("total", 1) * 100
        scores["concurrent"] = 1 if concurrent_success > 0 or mixed_success_rate >= 80 else 0
        logger.info(f"\nğŸŒ€ ë™ì‹œì„± ì²˜ë¦¬:")
        logger.info(f"   ë™ì¼ ê°ì²´ ìˆ˜ì • ì„±ê³µ: {concurrent_success}/100")
        logger.info(f"   í˜¼í•© ì‘ì—… ì„±ê³µë¥ : {mixed_success_rate:.1f}%")
        logger.info(f"   ì ìˆ˜: {'âœ… í†µê³¼' if scores['concurrent'] else 'âŒ ì‹¤íŒ¨'}")
        
        # ë³µêµ¬ ëŠ¥ë ¥ ì ìˆ˜
        recovery = test_results.get("recovery", {})
        scores["recovery"] = 1 if recovery.get("recovered", False) else 0
        logger.info(f"\nğŸ”§ ì¥ì•  ë³µêµ¬:")
        logger.info(f"   ë³µêµ¬ ì‹œê°„: {recovery.get('recovery_time', 0):.2f}ì´ˆ")
        logger.info(f"   ì ìˆ˜: {'âœ… í†µê³¼' if scores['recovery'] else 'âŒ ì‹¤íŒ¨'}")
        
        # 3. ìµœì¢… ì ìˆ˜
        total_score = sum(scores.values())
        max_score = len(scores)
        percentage = (total_score / max_score) * 100
        
        logger.info("\n" + "="*80)
        logger.info(f"ğŸ† ìµœì¢… ì ìˆ˜: {total_score}/{max_score} ({percentage:.0f}%)")
        logger.info("="*80)
        
        # 4. í”„ë¡œë•ì…˜ ì¤€ë¹„ë„ í‰ê°€
        logger.info("\n### 3. í”„ë¡œë•ì…˜ ì¤€ë¹„ë„ í‰ê°€")
        
        if percentage >= 75:
            logger.info("âœ… í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ")
            logger.info("   OMSëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif percentage >= 50:
            logger.info("âš ï¸ ì¡°ê±´ë¶€ í”„ë¡œë•ì…˜ ê°€ëŠ¥")
            logger.info("   ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•˜ì§€ë§Œ ì œí•œì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            logger.info("âŒ ì¶”ê°€ ê°œë°œ í•„ìš”")
            logger.info("   í”„ë¡œë•ì…˜ ë°°í¬ ì „ ì£¼ìš” ë¬¸ì œ í•´ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
        # 5. ê¶Œì¥ì‚¬í•­
        logger.info("\n### 4. ê°œì„  ê¶Œì¥ì‚¬í•­")
        
        recommendations = []
        
        if not scores["burst"]:
            recommendations.append("- API ì‘ë‹µ ì†ë„ ìµœì í™” (ìºì‹±, ì—°ê²° í’€ë§)")
            
        if not scores["event"]:
            recommendations.append("- ì´ë²¤íŠ¸ ì²˜ë¦¬ ì„±ëŠ¥ ê°œì„  (ë°°ì¹˜ ì²˜ë¦¬, ë¹„ë™ê¸° ìµœì í™”)")
            
        if not scores["concurrent"]:
            recommendations.append("- ë™ì‹œì„± ì œì–´ ë©”ì»¤ë‹ˆì¦˜ ê°•í™” (ë‚™ê´€ì  ì ê¸ˆ, ì¶©ëŒ í•´ê²°)")
            
        if not scores["recovery"]:
            recommendations.append("- ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ê°œì„  (Circuit Breaker, ìë™ ìŠ¤ì¼€ì¼ë§)")
            
        for rec in recommendations:
            logger.info(rec)
            
        if not recommendations:
            logger.info("- í˜„ì¬ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ ì§€ì† ê¶Œì¥")
            
        # 6. ìƒì„¸ ë©”íŠ¸ë¦­ íŒŒì¼ ì €ì¥
        logger.info("\n### 5. ìƒì„¸ ê²°ê³¼ ì €ì¥")
        
        detailed_results = {
            "test_time": datetime.now().isoformat(),
            "duration_seconds": total_time,
            "test_results": test_results,
            "scores": scores,
            "final_score": f"{total_score}/{max_score}",
            "percentage": percentage,
            "production_ready": percentage >= 75,
            "metrics_summary": {
                "total_api_calls": len(self.metrics["api_latencies"]),
                "total_events": len(self.metrics["event_latencies"]),
                "total_errors": len(self.metrics["errors"])
            }
        }
        
        async with aiofiles.open("chaos_test_detailed_results.json", "w") as f:
            await f.write(json.dumps(detailed_results, indent=2, ensure_ascii=False))
            
        logger.info("ğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ chaos_test_detailed_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        logger.info("ğŸ“ ì „ì²´ ë¡œê·¸ëŠ” chaos_test_detailed.logì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    async def cleanup(self):
        """ì •ë¦¬ ì‘ì—…"""
        logger.info("\nì •ë¦¬ ì‘ì—… ìˆ˜í–‰ ì¤‘...")
        await self.nc.close()
        for client in self.http_clients:
            await client.aclose()
        await self.db.disconnect()
        logger.info("âœ… ì •ë¦¬ ì™„ë£Œ")
        
    async def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        await self.setup()
        
        test_results = {}
        
        # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results["burst_load"] = await self.chaos_test_1_burst_load()
        await asyncio.sleep(2)
        
        test_results["event_storm"] = await self.chaos_test_2_event_storm()
        await asyncio.sleep(2)
        
        test_results["concurrent_chaos"] = await self.chaos_test_3_concurrent_chaos()
        await asyncio.sleep(2)
        
        test_results["recovery"] = await self.chaos_test_4_recovery()
        
        # ìµœì¢… ë³´ê³ ì„œ
        await self.generate_final_report(test_results)
        
        await self.cleanup()


async def main():
    test = DetailedChaosTest()
    await test.run_all_tests()


if __name__ == "__main__":
    print("ğŸš€ OMS ì¹´ì˜¤ìŠ¤ E2E ì„±ëŠ¥ ê²€ì¦ (ìƒì„¸ ë¡œê·¸ ë²„ì „)")
    print("ëª¨ë“  ë¡œê·¸ëŠ” chaos_test_detailed.log íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.")
    print("="*80)
    asyncio.run(main())